{
 "metadata": {
  "name": "data_wrangling_page"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction\n",
      "1. Clean data is pushed to the google cloud.  \n",
      "    1.  Collect log data from sources (local, S3)  \n",
      "    1.  Amalgamate log files into task    \n",
      "    1.  Push to cloud (Google)  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load_ext autoreload\n",
      "#%autoreload 2\n",
      "\n",
      "from collections import defaultdict, OrderedDict\n",
      "from itertools import groupby\n",
      "import httplib2\n",
      "import sys\n",
      "import json\n",
      "import pprint\n",
      "from apiclient.discovery import build\n",
      "from oauth2client.file import Storage\n",
      "from oauth2client.client import AccessTokenRefreshError\n",
      "from oauth2client.client import OAuth2WebServerFlow\n",
      "from oauth2client.tools import run\n",
      "from apiclient.errors import HttpError\n",
      "from fetch.flow import FLOW\n",
      "from scipy import cluster\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "#Global Vars\n",
      "storage_location = 'bigquery_web.dat'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named httplib2",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-ac5d2fe9cf0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhttplib2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named httplib2"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Amalgamate Logs:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Normalize TimData"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#(rows,cols) = features.shape\n",
      "        #normFeatures = np.zeros((rows,cols))\n",
      "\n",
      "        #print rows, cols\n",
      "        #for col in range(cols):\n",
      "         #   feature = features[:,col]\n",
      "        \n",
      "            #Retrieve 2nd and 98th thresholds\n",
      "            #lowNormT =  float(thresholds[sensors[col]]['lowNorm'])\n",
      "            #highNormT = float(thresholds[sensors[col]]['highNorm'])\n",
      "    \n",
      "            #Perform normalization as based on Tim Code (i.e. Map 2nd and 98th perc. to [-1 1]\n",
      "            #features[:,col] = (feature - lowNormT)* (2/(highNormT-lowNormT)) - 1\n",
      "    \n",
      "        #normFeatures now normalized version of features '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = '''def normalize(task, col, thresholds):\n",
      "    qsCol = \"SELECT {0} FROM [data.{1}] WHERE task='{2}'\".format(col, table_name, task)\n",
      "    results = queryGoogle(qsCol)\n",
      "    feature = (np.array([[float(field['v']) for field in row['f']]for row in results['rows']]))\n",
      "    lowNormT = float(thresholds[task][col]['lowNorm'])\n",
      "    highNormT = float(thresholds[task][col]['highNorm'])\n",
      "    normal = (feature - lowNormT)* (2/(highNormT-lowNormT)) - 1\n",
      "    return normal.tolist()\n",
      "    \n",
      "def normalizeTimData():\n",
      "    table_name = 'timdata'\n",
      "    import numpy as np\n",
      "    tasks = ['cutting', 'suturing', 'pegtransfer']# ['cutting'] #\n",
      "    columns = [field['name'] for field in json.loads(getSchemaFields(table_name))][4:]\n",
      "    thresholds = getThresholds(table_name)  \n",
      "    features = defaultdict(list)\n",
      "    for task in tasks: \n",
      "        for col in columns:\n",
      "            normed = normalize(task, col, thresholds[task])\n",
      "            features[task].append(normed)\n",
      "            \n",
      "            '''\n",
      "z='''\n",
      "            qsCol = \"SELECT {0} FROM [data.{1}] WHERE task='{2}'\".format(col, table_name, task)\n",
      "            results = queryGoogle(qsCol)\n",
      "            feature = (np.array([[float(field['v']) for field in row['f']]for row in results['rows']]))\n",
      "            lowNormT = float(thresholds[task][col]['lowNorm'])\n",
      "            highNormT = float(thresholds[task][col]['highNorm'])\n",
      "            normal = (feature - lowNormT)* (2/(highNormT-lowNormT)) - 1\n",
      "            print 'This is normal:::\\n', normal[:10]\n",
      "            df = pd.DataFrame(normal, columns=[col])'''\n",
      "q='''          \n",
      "            features.join(df, how='outer')\n",
      "            print features.head()\n",
      "            print features.shape\n",
      "                \n",
      "    \n",
      "\n",
      "    return features'''\n",
      "    \n",
      "#x = normalizeTimData()\n",
      "#print x[:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def qOutliers(task, table_name, thresholds=None, columns=None):\n",
      "    thresholds = thresholds if thresholds else {task: getThresholds(table_name, task)}\n",
      "    columns = columns if columns else [field['name'] for field in json.loads(getSchemaFields(table_name))]\n",
      "    sensors = columns[4:]\n",
      "    SELECT = (\"SELECT \") #+ (', '.join(columns[:4])) + ',' \n",
      "    select = [] \n",
      "    FROM = (\" FROM [data.\"+ table_name +\"] \")\n",
      "    WHERE = (\"WHERE task='{0}' AND \".format(task))\n",
      "    where = []\n",
      "    for sensor in sensors:\n",
      "        select.append(sensor)\n",
      "        if abs(float(thresholds[task][sensor]['lowNorm']) - float(thresholds[task][sensor]['highNorm'])) > 0.01 or \\\n",
      "           abs(float(thresholds[task][sensor]['lowOutlier']) - float(thresholds[task][sensor]['highOutlier'])) > 0.01:\n",
      "            where.append(\"({0} > {1} AND {0} < {2})\\n\".format(sensor, \n",
      "                                                             thresholds[task][sensor]['lowOutlier'], \n",
      "                                                             thresholds[task][sensor]['highOutlier']))\n",
      "    \n",
      "    return SELECT + (', '.join(select)) + FROM + WHERE + ' AND '.join(where) \n",
      "\n",
      "#print qOutliers('cutting', 'timdata')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getThresholds(table_name, task_type=None):\n",
      "    #sensors = getSensors(table_name)\n",
      "    thresholds = defaultdict(lambda: defaultdict(dict))\n",
      "    data = queryTableData('data', 'thresholds')\n",
      "    \n",
      "    for row in data['rows']:\n",
      "        cells = row['f']\n",
      "        task = cells[0]['v']\n",
      "        ttype = cells[2]['v']\n",
      "        sensor = cells[3]['v']\n",
      "        thresholds[task][sensor][ttype] = cells[4]['v']\n",
      "    if task_type:\n",
      "        return thresholds.get(task_type, 'ERROR: task not found')\n",
      "    \n",
      "    return thresholds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Upload TimData\n",
      "upload tim's data from his thesis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "import time\n",
      "def uploadTim():\n",
      "    table_name = 'timdata'\n",
      "    fields = getSchemaFields(table_name).replace('-', '')\n",
      "    def task_parse(name):\n",
      "        n = name.lower()\n",
      "        if 'suturing' in n: return 'suturing'\n",
      "        if 'pegtx' in n or 'pegboard' in n: return 'pegtransfer'\n",
      "        if 'cutting' in n: return 'cutting'\n",
      "        return 'unknown'\n",
      "    #data = fetchLocalData('/home/ubuntu/simulab/data/CSVs', task_parse)\n",
      "    #data = open('timdata_csv_387').readlines()\n",
      "    file_names = glob.glob('/home/ubuntu/simulab/small*')\n",
      "    print 'got data', len(file_names), ' files'\n",
      "    #print file_names\n",
      "    for fname in file_names:\n",
      "        with open(fname) as data:\n",
      "            print fname, '...',\n",
      "            body = getBody(data.read(), fields, table_name, dataset\n",
      "                                  , createDisposition='CREATE_IF_NEEDED'\n",
      "                                  , writeDisposition='WRITE_APPEND')\n",
      "    \n",
      "            loadTableFromCSV(body)\n",
      "        print 'uploaded'\n",
      "        time.sleep(120)\n",
      "#uploadTim()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#FETCH:  \n",
      "\n",
      "Fetches all data from Google BigQuery or Amazon s3\n",
      "Code to connect, clean, shape\n",
      "and move data\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Fetch-Local\n",
      "amalgamates all files in a local directory and prepares for upload into google bigquery"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "import os\n",
      "import cStringIO \n",
      "\n",
      "\n",
      "def fetchLocalData(path, task_parse):\n",
      "    globPath = path + '/*'\n",
      "    data = open('timdata_csv_387', 'w+')#cStringIO.StringIO()\n",
      "    file_names = glob.glob(globPath)\n",
      "    print len(file_names)\n",
      "    for fname in file_names:\n",
      "        fkey = fname[:-4]\n",
      "        task = task_parse(fname)\n",
      "        fdate = '2012-08-30'\n",
      "        with open(fname) as fin:\n",
      "            for line in fin.readlines():\n",
      "                if line.strip():\n",
      "                    data.write('{0},{1},{2},{3}\\n'.format(fkey, task, fdate, line.strip()))\n",
      "    data.seek(0)\n",
      "    return data\n",
      "    \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Fetch-Google"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "projectId = '459512762119'\n",
      "dataset = 'data'\n",
      "url = \"https://www.googleapis.com/upload/bigquery/v2/projects/\" + projectId + \"/jobs\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Query using BigQuery syntax"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "{'rows': []}\n",
      "job complete\n",
      "current length:  10\n",
      "1\n",
      "10\n",
      "{'rows': [{u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}, {u'f': [{u'v': u'-0.432'}, {u'v': u'3.655'}]}]}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def queryGoogle(queryString, http=None):  \n",
      "    http = http if http else httpGoogle()\n",
      "    data = {'rows': []}\n",
      "    timeout = 70000 \n",
      "    service = build(\"bigquery\", \"v2\", http=http)\n",
      "    jobCollection = service.jobs()\n",
      "    queryData = {'query': queryString,\n",
      "                 'timeoutMs': timeout,\n",
      "                 'maxResults': 100000}\n",
      "    try:\n",
      "        queryReply = jobCollection.query(projectId=projectId,\n",
      "                                         body=queryData).execute()\n",
      "        \n",
      "        jobReference=queryReply['jobReference']\n",
      "        #print jobReference['jobId']\n",
      "        #print projectId\n",
      "        # Timeout exceeded: keep polling until the job is complete.\n",
      "        while(not queryReply['jobComplete']):\n",
      "            print 'Job not yet complete...'\n",
      "            queryReply = jobCollection.getQueryResults(\n",
      "                              projectId=jobReference['projectId'],\n",
      "                              jobId=jobReference['jobId'],\n",
      "                              timeoutMs=timeout).execute() \n",
      "            print projectId, ':', jobId\n",
      "            print 'job complete'\n",
      "        #get first page of results\n",
      "        if('rows' in queryReply):\n",
      "                data['rows'].extend(queryReply['rows'])\n",
      "                currentRow = len(queryReply['rows'])\n",
      "                print 'current length: ', currentRow\n",
      "        #check for additional pages   \n",
      "        i = 0\n",
      "        while('rows' in queryReply and currentRow < queryReply['totalRows']):\n",
      "            i += 1\n",
      "            print i, \n",
      "            queryReply = jobCollection.getQueryResults(\n",
      "                         projectId=jobReference['projectId'],\n",
      "                         jobId=jobReference['jobId'],\n",
      "                         startIndex=currentRow).execute()\n",
      "            \n",
      "            if('rows' in queryReply):\n",
      "                data['rows'].extend(queryReply['rows'])\n",
      "                currentRow += len(queryReply['rows'])\n",
      "                print 'current row: ', currentRow\n",
      "        print 'returning data'\n",
      "        return data\n",
      "        print (\"The credentials have been revoked or expired, please re-run\"\n",
      "        \"the application to re-authorize\")\n",
      "    \n",
      "    except HttpError as err:\n",
      "        print 'Error in runSyncQuery:', pprint.pprint(err.content)\n",
      "    \n",
      "    except Exception as err:\n",
      "        print 'Undefined error: ',  err\n",
      "\n",
      "\n",
      "#x= queryGoogle(qs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Dump entire table into rows"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def queryTableData(dataset, table, startIndex=0, http=None):\n",
      "    http = http if http else httpGoogle()\n",
      "    data = {'rows': []}\n",
      "    service = build(\"bigquery\", \"v2\", http=http)\n",
      "    tableDataJob = service.tabledata()\n",
      "    try:\n",
      "        queryReply = tableDataJob.list(projectId=projectId,\n",
      "                                     datasetId=dataset,\n",
      "                                     tableId=table,\n",
      "                                     startIndex=startIndex).execute()\n",
      "        \n",
      "        # When we've printed the last page of results, the next\n",
      "        # page does not have a rows[] array.\n",
      "        while 'rows' in queryReply:\n",
      "            data['rows'].extend(queryReply['rows'])\n",
      "            startIndex += len(queryReply['rows'])\n",
      "            queryReply = tableDataJob.list(projectId=projectId,\n",
      "                                     datasetId=dataset,\n",
      "                                     tableId=table,\n",
      "                                     startIndex=startIndex).execute()\n",
      "            \n",
      "        return data\n",
      "    except HttpError as err:\n",
      "        print 'Error in querytableData: ', pprint.pprint(err.content)\n",
      "        \n",
      "#print queryTableData('data', 'thresholds')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Upload data to table using csv file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadTableFromCSV(bodydata, http=None):\n",
      "    http = http if http else httpGoogle()\n",
      "    headers = {'Content-Type': 'multipart/related; boundary=xxx'}\n",
      "    res, content = http.request(url, method=\"POST\", body=bodydata, headers=headers)\n",
      "    print str(res) + \"\\n\"\n",
      "    print content"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Fetch-Authorization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def httpGoogle(storage_location='bigquery_web.dat' ):\n",
      "    storage = Storage(storage_location) \n",
      "    credentials = storage.get()\n",
      "    if credentials is None or credentials.invalid:\n",
      "        print '''There is a manual step to updating credentials.\n",
      "                 Please follow Reauthorize instructions\n",
      "              ''' \n",
      "    \n",
      "    return credentials.authorize(httplib2.Http())\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Utilities:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Google"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Fetch fields from schema table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getSchemaFields(table_name):\n",
      "    queryString = \"SELECT field_name, field_type, field_order FROM [data.schemata] WHERE table_name = '{0}' ORDER BY field_order\".format(table_name)\n",
      "    results = queryGoogle(queryString)\n",
      "    fields = ['{{\"name\":\"{0}\", \"type\":\"{1}\"}}'.format(row['f'][0]['v'].strip(), row['f'][1]['v'].strip()) for row in results['rows']]\n",
      "    return  str(('\\n            \\t[') + (',\\n            \\t'.join(fields)) + ('\\n            \\t]'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Create Schemata \n",
      "create schema table from schemat.dat\n",
      "***Will truncate so append schemata.dat only***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createSchemata(path='/home/ubuntu/simulab/data/schemata.dat'):\n",
      "    datafile = open(path).read()\n",
      "\n",
      "    fields = '''[ \n",
      "              {\"name\" : \"field_order\", \"type\" : \"INTEGER\"}\n",
      "            , {\"name\" : \"table_name\",  \"type\" : \"STRING\"}\n",
      "            , {\"name\" : \"field_name\",  \"type\" : \"STRING\"}\n",
      "            , {\"name\" : \"field_type\",  \"type\" : \"STRING\"}\n",
      "                  ]'''\n",
      "    body = getBody(datafile, fields, 'schemata', 'data', 'CREATE_IF_NEEDED' , 'WRITE_TRUNCATE')\n",
      "    loadTableFromCSV(body)\n",
      "    return body\n",
      "\n",
      "#x = createSchemata()\n",
      "#print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Create table schema and body of request"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getBody(data, fields, table, dataset, createDisposition, writeDisposition):\n",
      "    bodyEnd = ('\\n--xxx--\\n')\n",
      "    strbody = '''--xxx\n",
      "Content-Type: application/json; charset=UTF-8\n",
      "\n",
      "{\n",
      "   \"configuration\": {\n",
      "      \"load\": {\n",
      "         \"schema\": {\n",
      "            \"fields\" : %s\n",
      "         },\n",
      "         \"destinationTable\": {\n",
      "            \"projectId\": \"%s\", \n",
      "            \"datasetId\": \"%s\",\n",
      "            \"tableId\": \"%s\" \n",
      "            \n",
      "         }, \n",
      "         \"createDisposition\": \"%s\", \n",
      "         \"writeDisposition\": \"%s\",\n",
      "         \"fieldDelimiter\": \",\"\n",
      "      }\n",
      "   }\n",
      "}\n",
      "--xxx\n",
      "Content-Type: application/octet-stream\n",
      "\n",
      "''' %(fields, projectId, dataset, table, createDisposition, writeDisposition)\n",
      "\n",
      "    return (strbody) + (data) + bodyEnd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Reauthorize (Create valid credentials):\n",
      "Google's authentication scheme doesn't work on text based browser (it does and is extremely painful)\n",
      "To create the credentials it's much easier to fake them out (fakie).  \n",
      "The fakie string below contains the string representation of stored (json) authentication keys.  Once   \n",
      "created these credentials should work on any machine for any purpose designated by the google console  \n",
      "(web, installed device)\n",
      "In order to create new credentials you will need access to:  \n",
      "1. [Google API Console](https://code.google.com/apis/console)   \n",
      "2. [OAuth Playground](https://developers.google.com/oauthplayground)   \n",
      "3. rich browser (chrome).\n",
      "\n",
      "*[Google Blog walk-through](http://googleappsdeveloper.blogspot.com/2011/09/python-oauth-20-google-data-apis.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "         "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Google Setup:  \n",
      "1. If the appropriate client id does not exsit, in the [Google API Console](https://code.google.com/apis/console):\n",
      "\n",
      "     1.  Click Create Another Client ID  \n",
      "     1.  Select 'Web Application' for web access /  Installed Applications  for console apps just select Other and ok.  \n",
      "     1.  Update the default to https://localhost:8888/oauth2callback for web access / click 'Other' for console apps  \n",
      "     1.  Click ok  \n",
      "\n",
      "1. From the [Google API Console](https://code.google.com/apis/console) update variables:\n",
      "     *  client_id  \n",
      "     *  client_secret  \n",
      "     *  redirect_uri "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Authorization  \n",
      "1.  Run the code below\n",
      "1.  Paste auth_uri into rich browser\n",
      "    1.  Authorize simscore\n",
      "    1.  Click through any SSL certification errors\n",
      "    1.  Copy code from address bar (after code=)\n",
      "    1.  Paste the value into the variable code below\n",
      "1.  Run the code below again\n",
      "\n",
      "You should now be authenticated.  The storage_location under global variables holds the credentials files read in by all the functions.  If you change this then other functions won't be able to find the credentials file saved by this.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This token only works once.\n",
      "code = '4/Q1Fc8XZ5NDKZAn3AUk9b6pKhv2b0.Qn7hMbdRNIYXuJJVnL49Cc8oCnj6cgI'\n",
      "\n",
      "\n",
      "client_id = '459512762119-hnjnbr6nilcuib72ig1tk4pom3bfefgr.apps.googleusercontent.com'\n",
      "client_secret = 'gIy7bJaFm6CRX7cXUoCQdB74'\n",
      "redirect_uri = 'https://localhost:8888/oauth2callback'\n",
      "scope = 'https://www.googleapis.com/auth/bigquery'\n",
      "token_uri='https://accounts.google.com/o/oauth2/token'\n",
      "def get_credentials():\n",
      "    flow = OAuth2WebServerFlow(client_id=client_id,\n",
      "                              client_secret=client_secret,\n",
      "                              scope=scope,\n",
      "                              redirect_uri=redirect_uri)\n",
      "    flow.params['access_type'] = 'offline'\n",
      "    flow.params['approval_prompt'] = 'force'\n",
      "\n",
      "    auth_uri = flow.step1_get_authorize_url(redirect_uri)\n",
      "    print 'auth_uri: \\n', auth_uri #navigate to uri authorize and update code above. then rerun this.  You should be authenticated.\n",
      "    \n",
      "    credential = flow.step2_exchange(code)\n",
      "        \n",
      "    storage = Storage(storage_location) \n",
      "    storage.put(credential)\n",
      "    credential.set_store(storage)\n",
      "    print 'Authentication successful.'\n",
      "\n",
      "    return credential"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    }
   ],
   "metadata": {}
  }
 ]
}