{
 "metadata": {
  "name": "",
  "signature": "sha256:82042c73554e12998301b0336aa04ca287bd15519d1150ea8043adb903dda84a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Google Latitude Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### To Do - make this a narrative fitting of a blog\n",
      "1. ~~Plot the chloropleth of neighborhoods~~\n",
      "2. Remove time at work and home from data and re-plot\n",
      "3. Find farthest point traveled\n",
      "4. ~~Calculate number of flights taken~~\n",
      "\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import fiona\n",
      "from itertools import chain\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "import pandas as pd\n",
      "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
      "from shapely.prepared import prep\n",
      "import matplotlib.cm as cm\n",
      "from matplotlib.collections import PatchCollection\n",
      "from descartes import PolygonPatch\n",
      "import json\n",
      "import datetime\n",
      "\n",
      "import helpers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance_on_unit_sphere(lat1, long1, lat2, long2):\n",
      "    import math\n",
      "    # http://www.johndcook.com/python_longitude_latitude.html\n",
      "    # Convert latitude and longitude to \n",
      "    # spherical coordinates in radians.\n",
      "    degrees_to_radians = math.pi/180.0  \n",
      "    # phi = 90 - latitude\n",
      "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
      "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
      "    # theta = longitude\n",
      "    theta1 = long1*degrees_to_radians\n",
      "    theta2 = long2*degrees_to_radians\n",
      "        \n",
      "    # Compute spherical distance from spherical coordinates.\n",
      "    # For two locations in spherical coordinates \n",
      "    # (1, theta, phi) and (1, theta, phi)\n",
      "    # cosine( arc length ) = \n",
      "    #    sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n",
      "    # distance = rho * arc length\n",
      "    \n",
      "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + \n",
      "           math.cos(phi1)*math.cos(phi2))\n",
      "    arc = math.acos( cos )\n",
      "\n",
      "    # Remember to multiply arc by the radius of the earth \n",
      "    # in your favorite set of units to get length.\n",
      "    return arc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convenience functions for working with colour ramps and bars\n",
      "def colorbar_index(ncolors, cmap, labels=None, **kwargs):\n",
      "    \"\"\"\n",
      "    This is a convenience function to stop you making off-by-one errors\n",
      "    Takes a standard colourmap, and discretises it,\n",
      "    then draws a color bar with correctly aligned labels\n",
      "    \"\"\"\n",
      "    cmap = cmap_discretize(cmap, ncolors)\n",
      "    mappable = cm.ScalarMappable(cmap=cmap)\n",
      "    mappable.set_array([])\n",
      "    mappable.set_clim(-0.5, ncolors+0.5)\n",
      "    colorbar = plt.colorbar(mappable, **kwargs)\n",
      "    colorbar.set_ticks(np.linspace(0, ncolors, ncolors))\n",
      "    colorbar.set_ticklabels(range(ncolors))\n",
      "    if labels:\n",
      "        colorbar.set_ticklabels(labels)\n",
      "    return colorbar\n",
      "\n",
      "def cmap_discretize(cmap, N):\n",
      "    \"\"\"\n",
      "    Return a discrete colormap from the continuous colormap cmap.\n",
      "\n",
      "        cmap: colormap instance, eg. cm.jet. \n",
      "        N: number of colors.\n",
      "\n",
      "    Example\n",
      "        x = resize(arange(100), (5,100))\n",
      "        djet = cmap_discretize(cm.jet, 5)\n",
      "        imshow(x, cmap=djet)\n",
      "\n",
      "    \"\"\"\n",
      "    if type(cmap) == str:\n",
      "        cmap = get_cmap(cmap)\n",
      "    colors_i = np.concatenate((np.linspace(0, 1., N), (0., 0., 0., 0.)))\n",
      "    colors_rgba = cmap(colors_i)\n",
      "    indices = np.linspace(0, 1., N + 1)\n",
      "    cdict = {}\n",
      "    for ki, key in enumerate(('red', 'green', 'blue')):\n",
      "        cdict[key] = [(indices[i], colors_rgba[i - 1, ki], colors_rgba[i, ki]) for i in xrange(N + 1)]\n",
      "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\" % N, cdict, 1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Load Latitude data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    fh = open(r'C:\\Users\\Tyler\\Documents\\My Dropbox\\LocationHistory_8_18_14.json')\n",
      "except:\n",
      "    fh = open(r'C:\\Users\\thartley\\Documents\\Dropbox\\LocationHistory_8_18_14.json')\n",
      "buf = fh.read()\n",
      "raw = json.loads(buf)\n",
      "fh.close()\n",
      "\n",
      "ld = pd.DataFrame(raw['locations'])\n",
      "ld.rename(columns={'latitudeE7':'latitude', 'longitudeE7':'longitude', 'timestampMs':'timestamp'}, inplace=True)\n",
      "del raw\n",
      "ld['latitude'] = ld['latitude']/float(1e7)\n",
      "ld['longitude'] = ld['longitude']/float(1e7)\n",
      "ld['timestamp'] = ld['timestamp'].map(lambda x: float(x)/1000)\n",
      "ld['datetime'] = ld.timestamp.map(datetime.datetime.fromtimestamp)\n",
      "ld = ld[ld.timestamp > 1374303600.0] #time since Jul. 20, 2013 when data reporting increased\n",
      "ld = ld[ld.accuracy < 1000] #Ignore locations with location estimates over 1000m?\n",
      "ld.reset_index(drop=True, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import Seattle Shapefile data and start making Polygons"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#shp = fiona.open(r'C:\\Users\\thartley\\Documents\\Seattle_20City_20Limits\\WGS84\\Seattle City Limits')\n",
      "#r'C:\\Users\\thartley\\Downloads\\london\\london_wards'\n",
      "shapefilename = helpers.user_prefix() + r'data\\Neighborhoods\\WGS84\\Neighborhoods'\n",
      "#shapefilename = helpers.user_prefix() + r'data\\Shorelines\\WGS84\\Shorelines'\n",
      "\n",
      "shp = fiona.open(shapefilename+'.shp')\n",
      "bds = shp.bounds\n",
      "shp.close()\n",
      "extra = 0.01\n",
      "ll = (bds[0], bds[1])\n",
      "ur = (bds[2], bds[3])\n",
      "coords = list(chain(ll, ur))\n",
      "w, h = coords[2] - coords[0], coords[3] - coords[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = Basemap(\n",
      "    projection='tmerc',\n",
      "    lon_0=np.mean([coords[0], coords[2]]),\n",
      "    lat_0=np.mean([coords[1], coords[3]]),\n",
      "    ellps = 'WGS84',\n",
      "    llcrnrlon=coords[0] - extra * w,\n",
      "    llcrnrlat=coords[1] - (0.08 * h), \n",
      "    urcrnrlon=coords[2] + extra * w,\n",
      "    urcrnrlat=coords[3] + (extra+0.01 * h),\n",
      "    resolution='i',\n",
      "    suppress_ticks=True)\n",
      "# Import the shapefile data to the Basemap \n",
      "out = m.readshapefile(shapefilename, 'seattle', drawbounds=False, color='none', zorder=2)\n",
      "\n",
      "zzz =\"\"\"\n",
      "m2 = Basemap(\n",
      "    projection='tmerc',\n",
      "    lon_0=-122.3,\n",
      "    lat_0=47.6,\n",
      "    #lon_0=-2.,\n",
      "    #lat_0=49.,\n",
      "    ellps = 'WGS84',\n",
      "    llcrnrlon=coords[0] - extra * w,\n",
      "    llcrnrlat=coords[1] - extra + 0.01 * h,\n",
      "    urcrnrlon=coords[2] + extra * w,\n",
      "    urcrnrlat=coords[3] + extra + 0.01 * h,\n",
      "    lat_ts=0,\n",
      "    resolution='i',\n",
      "    suppress_ticks=True)\n",
      "out = m2.readshapefile(\n",
      "    helpers.user_prefix() + r'data\\Shorelines\\WGS84\\Shorelines',\n",
      "    'water',\n",
      "    color='none',\n",
      "    zorder=2)\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set up a map dataframe\n",
      "df_map = pd.DataFrame({\n",
      "    'poly': [Polygon(xy) for xy in m.seattle],\n",
      "    'name': [nhood['S_HOOD'] for nhood in m.seattle_info]})\n",
      "df_map['area_m'] = df_map['poly'].map(lambda x: x.area)\n",
      "df_map['area_km'] = df_map['area_m'] / 100000\n",
      "#df_map['poly'].append(Polygon(m2.water[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create Point objects in map coordinates from dataframe lon and lat values\n",
      "map_points = pd.Series([Point(m(mapped_x, mapped_y)) for mapped_x, mapped_y in zip(ld['longitude'], ld['latitude'])])\n",
      "\n",
      "all_points = MultiPoint(list(map_points.values))\n",
      "hood_polygons = prep(MultiPolygon(list(df_map['poly'].values)))\n",
      "\n",
      "city_points = filter(hood_polygons.contains, all_points)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Neighborhood Chloropleth Graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute the points that belong in each neighborhood\n",
      "def num_of_contained_points(apolygon, city_points):\n",
      "    return int(len(filter(prep(apolygon).contains, city_points)))\n",
      "    \n",
      "df_map['hood_count'] = df_map['poly'].apply(num_of_contained_points, args=(city_points,))\n",
      "#df_map['hood_count'] = df_map['poly'].map(lambda x: int(len(filter(prep(x).contains, city_points))))b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_map['hood_perc'] = df_map.hood_count/df_map.hood_count.sum()\n",
      "df_map['hood_hours'] = df_map.hood_count/60."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pysal.esda.mapclassify import Natural_Breaks\n",
      "from matplotlib.colors import Normalize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate Jenks natural breaks for density\n",
      "breaks = Natural_Breaks(df_map[df_map['hood_hours'] > 0].hood_hours, initial=300, k=5)\n",
      "\n",
      "jb = pd.DataFrame({'jenks_bins': breaks.yb}, index=df_map[df_map.hood_count > 0].index)\n",
      "try:\n",
      "    df_map = df_map.join(jb)\n",
      "except:\n",
      "    df_map.jenks_bins = jb\n",
      "df_map.jenks_bins.fillna(-1, inplace=True)\n",
      "\n",
      "jenks_labels = [\"> %d hours\"%(perc) for perc in breaks.bins[:-1]]\n",
      "jenks_labels = ['Have never been here', \"> 0 hours\"]+jenks_labels\n",
      "print jenks_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "breaks = [0., 4., 24., 64., 135., 1e12]\n",
      "def self_categorize(entry, breaks):\n",
      "    for i in range(len(breaks)-1):\n",
      "        if entry > breaks[i] and entry <= breaks[i+1]:\n",
      "            return i\n",
      "    else:\n",
      "        return -1\n",
      "jb = df_map.hood_hours.apply(self_categorize, args=(breaks,))\n",
      "try:\n",
      "    df_map = df_map.join(jb)\n",
      "except:\n",
      "    df_map.jenks_bins = jb\n",
      "jenks_labels = [\"> %d hours\"%(perc) for perc in breaks[:-1]]\n",
      "jenks_labels = ['Have never been here']+jenks_labels\n",
      "print jenks_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "figwidth = 14\n",
      "fig = plt.figure(figsize=(figwidth, figwidth*h/w))\n",
      "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
      "\n",
      "# use a blue colour ramp - we'll be converting it to a map using cmap()\n",
      "cmap = plt.get_cmap('Blues')\n",
      "# draw wards with grey outlines\n",
      "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(x, ec='#111111', lw=.8, alpha=1., zorder=4))\n",
      "pc = PatchCollection(df_map['patches'], match_original=True)\n",
      "# impose our colour map onto the patch collection\n",
      "norm = Normalize()\n",
      "pc.set_facecolor(cmap(norm(df_map['jenks_bins'].values)))\n",
      "ax.add_collection(pc)\n",
      "\n",
      "# Add a colour bar\n",
      "cb = colorbar_index(ncolors=len(jenks_labels), cmap=cmap, shrink=0.5, labels=jenks_labels)\n",
      "cb.ax.tick_params(labelsize=16)\n",
      "\n",
      "\"\"\"\n",
      "# Bin method, copyright and source data info\n",
      "smallprint = ax.text(\n",
      "    1.03, 0,\n",
      "    'Classification method: natural breaks\\nContains Ordnance Survey data\\n$\\copyright$ Crown copyright and database right 2013\\nPlaque data from http://openplaques.org',\n",
      "    ha='right', va='bottom',\n",
      "    size=4,\n",
      "    color='#555555',\n",
      "    transform=ax.transAxes)\n",
      "\"\"\"\n",
      "# Draw a map scale\n",
      "m.drawmapscale(\n",
      "    coords[0] + 0.08, coords[1] + -0.002,\n",
      "    coords[0], coords[1],\n",
      "    10.,\n",
      "    fontsize=16,\n",
      "    barstyle='fancy', labelstyle='simple',\n",
      "    fillcolor1='w', fillcolor2='#555555',\n",
      "    fontcolor='#555555',\n",
      "    zorder=5)\n",
      "\n",
      "# this will set the image width to 722px at 100dpi\n",
      "plt.title(\"Time Spent in Seattle Neighborhoods\", fontsize=16)\n",
      "plt.tight_layout()\n",
      "plt.savefig('data/chloropleth.png', dpi=300, frameon=False, transparent=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hexbin Plot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"PLOT A HEXBIN MAP OF LOCATION\n",
      "\"\"\"\n",
      "\n",
      "helpers.tic()\n",
      "\n",
      "# draw ward patches from polygons\n",
      "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
      "    x, fc='#555555', ec='#555555', lw=1, alpha=1, zorder=0))\n",
      "\n",
      "plt.clf()\n",
      "figwidth = 14\n",
      "fig = plt.figure(figsize=(figwidth, figwidth*h/w))\n",
      "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
      "\n",
      "# plot boroughs by adding the PatchCollection to the axes instance\n",
      "ax.add_collection(PatchCollection(df_map['patches'].values, match_original=True))\n",
      "\n",
      "# the mincnt argument only shows cells with a value >= 1\n",
      "# hexbin wants np arrays, not plain lists\n",
      "hx = m.hexbin(\n",
      "    np.array([geom.x for geom in city_points]),\n",
      "    np.array([geom.y for geom in city_points]),\n",
      "    gridsize=(70, int(70*h/w)),\n",
      "    bins='log',\n",
      "    mincnt=1,\n",
      "    edgecolor='none',\n",
      "    alpha=1.,\n",
      "    cmap=plt.get_cmap('Blues'))\n",
      "\n",
      "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
      "    x, fc='none', ec='#FFFF99', lw=1, alpha=1, zorder=1))\n",
      "ax.add_collection(PatchCollection(df_map['patches'].values, match_original=True))\n",
      "\n",
      "# copyright and source data info\n",
      "smallprint = ax.text(\n",
      "    1.08, 0.0,\n",
      "    \"Google Latitude data from 2010-2014\\nProduced by Tyler Hartley\\nInspired by sensitivecities.com\",\n",
      "    ha='right', va='bottom',\n",
      "    size=figwidth/1.75,\n",
      "    color='#555555',\n",
      "    transform=ax.transAxes)\n",
      "\n",
      "# Draw a map scale\n",
      "m.drawmapscale(\n",
      "    coords[0] + 0.05, coords[1] - 0.01,\n",
      "    coords[0], coords[1],\n",
      "    4.,\n",
      "    units='mi',\n",
      "    barstyle='fancy', labelstyle='simple',\n",
      "    fillcolor1='w', fillcolor2='#555555',\n",
      "    fontcolor='#555555',\n",
      "    zorder=5)\n",
      "\n",
      "plt.title(\"Latitude Location History - Since 7/20/13\")\n",
      "#plt.tight_layout()\n",
      "# this will set the image width to 722px at 100dpi\n",
      "#fig.set_size_inches(7., 10.5)\n",
      "plt.savefig('data/location_history_7_20_13.png', dpi=300, frameon=False, transparent=True)\n",
      "\n",
      "helpers.toc()\n",
      "\n",
      "plt.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Flights Taken"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "degrees_to_radians = np.pi/180.0 \n",
      "ld['phi'] = (90.0 - ld.latitude) * degrees_to_radians \n",
      "ld['theta'] = ld.longitude * degrees_to_radians\n",
      "\n",
      "ld['distance'] = np.arccos( \n",
      "    np.sin(ld.phi)*np.sin(ld.phi.shift(-1)) * np.cos(ld.theta - ld.theta.shift(-1)) + \n",
      "    np.cos(ld.phi)*np.cos(ld.phi.shift(-1))\n",
      "    ) * 6378.100 # radius of earth in km\n",
      "\n",
      "ld['speed'] = ld.distance/(ld.timestamp - ld.timestamp.shift(-1))*3600\n",
      "\n",
      "# Identify potential flights\n",
      "travelindex = ld[(ld['speed'] > 10) & (ld['distance'] > 80.)].index\n",
      "print \"Found %s instances of flights\"%len(travelindex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make DataFrame of Flights taken with new columns: beginlat, beginlon, endlat, endlon, \n",
      "# begin, end, distance, speed\n",
      "\n",
      "flights = pd.DataFrame(data={'endlat':ld.ix[travelindex].latitude,\n",
      "                             'endlon':ld.ix[travelindex].longitude,\n",
      "                             'enddatetime':ld.ix[travelindex].datetime,\n",
      "                             'distance':ld.ix[travelindex].distance,\n",
      "                             'speed':ld.ix[travelindex].speed,\n",
      "                             },\n",
      "                       ).reset_index()\n",
      "flights['startlat'] = ld.ix[travelindex+1].latitude.reset_index(drop=True)\n",
      "flights['startlon'] = ld.ix[travelindex+1].longitude.reset_index(drop=True)\n",
      "flights['startdatetime'] = ld.ix[travelindex+1].datetime.reset_index(drop=True)\n",
      "\n",
      "# Clean up flights that have random GPS data in the middle of them\n",
      "f = flights[flights['index'].diff() == 1]\n",
      "cuts = np.split(f, (f['index'].diff() > 1).nonzero()[0])\n",
      "\n",
      "for cut in cuts:\n",
      "    flight_origin = cut.iloc[-1]\n",
      "    idx = cut.index[0]-1\n",
      "    flights.ix[idx, 'startlat'] = flight_origin.startlat\n",
      "    flights.ix[idx, 'startlon'] = flight_origin.startlon\n",
      "    flights.ix[idx, 'startdatetime'] = flight_origin.startdatetime\n",
      "    flights.ix[idx, 'startlat'] = flight_origin.startlat\n",
      "    flights.ix[idx, 'distance'] = distance_on_unit_sphere(flights.ix[idx].startlat,\n",
      "                                                       flights.ix[idx].startlon,\n",
      "                                                       flights.ix[idx].endlat,\n",
      "                                                       flights.ix[idx].endlon)*6378.1\n",
      "\n",
      "flights = flights.drop(f.index).reset_index(drop=True)\n",
      "flights = flights[flights.distance > 200].reset_index(drop=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(18,12))\n",
      "\n",
      "#fly = flights[(flights.startlon < 0) & (flights.endlon < 0)]# Western Hemisphere Flights\n",
      "fly = flights[(flights.startlon > 0) & (flights.endlon > 0)] # Eastern Hemisphere Flights\n",
      "#fly = flights # All flights. Need to use Robin projection w/ center as -180 as 2 cross 180/-180 Lon\n",
      "\n",
      "buf = .3\n",
      "minlat = np.min([fly.endlat.min(), fly.startlat.min()])\n",
      "minlon = np.min([fly.endlon.min(), fly.startlon.min()])\n",
      "maxlat = np.max([fly.endlat.max(), fly.startlat.max()])\n",
      "maxlon = np.max([fly.endlon.max(), fly.startlon.max()])\n",
      "width = maxlon - minlon\n",
      "height = maxlat - minlat\n",
      "\n",
      "\n",
      "m = Basemap(llcrnrlon=minlon - width*buf,\n",
      "            llcrnrlat=minlat - height*buf*1.5,\n",
      "            urcrnrlon=maxlon + width*buf,\n",
      "            urcrnrlat=maxlat + height*buf,\n",
      "            projection='merc', #'robin',\n",
      "            resolution='l',\n",
      "            #lat_1=minlat, lat_2=maxlat,\n",
      "            lat_0=minlat + height/2,\n",
      "            lon_0=minlon + width/2,)\n",
      "            #lon_0=-180)\n",
      "\n",
      "m.drawcoastlines()\n",
      "m.drawstates()\n",
      "m.drawcountries()\n",
      "m.fillcontinents()\n",
      "\n",
      "#m.drawparallels(np.arange(pts.latitude.min(),pts.latitude.max(),2.), labels=[1,1,1,1], fmt=\"%0.1f\")\n",
      "#m.drawmeridians(np.arange(pts.longitude.min(), pts.longitude.max(),5.), labels=[1,1,1,1], fmt=\"%0.1f\")\n",
      "\n",
      "for f in fly.iterrows():\n",
      "    f = f[1]\n",
      "    m.drawgreatcircle(f.startlon, f.startlat, f.endlon, f.endlat, linewidth=3, alpha=0.4, color='b' )\n",
      "    m.plot(*m(f.startlon, f.startlat), color='g', alpha=0.8, marker='o')\n",
      "    m.plot(*m(f.endlon, f.endlat), color='r', alpha=0.5, marker='o' )\n",
      "    #pa = Point(m(f.startlon, f.startlat))\n",
      "    #pb = Point(m(f.endlon, f.endlat))\n",
      "    #plt.plot([pa.x, pb.x], [pa.y, pb.y], linewidth=4)\n",
      "plt.savefig('data/flightdata.png', dpi=300, frameon=False, transparent=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Scratch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flights#.sort(columns='distance')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ld.ix[253164-5:253164+5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flights.ix[26,'index']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}